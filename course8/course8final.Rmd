---
title: "course8hw"
author: "Jimmy Nunnally"
date: "February 5, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#Purpose Build machine learning model to predict work out classes
#Load libraries
#I will be using random forest model due to prior knowledge and the fact that they generally perform well

```{r set   up}
library(caret)
library(randomForest)


test<-read.csv("pml-testing.csv")
train<-read.csv("pml-training.csv")

```


```{r EDA}

#Examine our features
head(test)
dim(train)

#Take a look at Our predictors
table(train$classe)
```


```{r preprocess}
#Preprocessing
# Partition our data into test/train/validation splits. Test/train is already done for us, so we will reserve 20% for validation and aim for a 60/20/20. Assuming that 20% is already reserved for test than this would be 75% of whats remaining for train and 25% for validation.This split is the standard used by data scientists in large data sets
#generating test/train split
## 75% of the sample size
set.seed(30)  #set set for reproducability
smp_size <- floor(0.75 * nrow(train))

train_ind <- sample(seq_len(nrow(train)), size = smp_size)
Training <- train[train_ind, ]
Validation<- train[-train_ind, ]
```

```{r feature select}
#Feature selection

#Exclude near zero variance features as they are not predictive
nzvcol <- nearZeroVar(Training)
Training <- Training[, -nzvcol]

# exclude features that are more than half NA values as this creates noise
cntlength <- sapply(Training, function(x) {
  sum(!(is.na(x) | x == ""))
})

NAcol <- names(cntlength[cntlength < 0.5 * length(Training$classe)])

Training <- Training[, !names(Training) %in% NAcol]

```

``` {r machine learning}
#Machine learning

Training$classe<-factor(Training$classe) 

#pick number of trees. I think 50 is a good balance between computation expense and predictive performance. Trial and error here

Model <- randomForest(classe ~ ., data = Training, importance = TRUE, ntrees = 50)

prediction <- predict(Model, Training)
print(confusionMatrix(prediction, Training$classe))

# Our model is perfect!!!! On the training data... but is it overfit?
```

```{r validation}

#validation

Validation$classe<-factor(Validation$classe) 

prediction_validation <- predict(Model, Validation)
print(confusionMatrix(prediction_validation, Validation$classe))

#Nope, still performs great!Hard to beat 99.99% accuracy

```

``` {r answers}
# Prediction/quiz answers 

quizanswers <- predict(Model, test)
print(quizanswers)

```




